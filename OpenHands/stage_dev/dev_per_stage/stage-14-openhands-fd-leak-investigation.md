# Stage 14: OpenHands Core File Descriptor Leak - Investigation and Fix

## Executive Summary

The "[Errno 24] Too many open files" error that crashed AWM after ~10 samples is caused by **multiple resource leaks in OpenHands core**, not in the AWM code. The Stage 13 fixes properly handle AWM-side cleanup, but the underlying OpenHands components leak file descriptors that accumulate across inference runs.

## Evidence

From the log of sample 11 (django__django-11039):
```
04:30:17 - openhands:INFO: pipeline.py:139 - Evaluation complete. Resolved: True
04:30:17 - openhands:ERROR: loop.py:203 - Error processing django__django-11039: ('Connection aborted.', OSError(24, 'Too many open files'))
```

**Key observation**: Sample 11 **PASSED evaluation** (Resolved: True) but failed during `runtime.close()` because file descriptors were already exhausted from samples 1-10.

## Root Causes Identified

### 1. EventStream Not Closed When Runtime Closes

**Location**: `openhands/core/setup.py:63` and `openhands/runtime/base.py`

**Problem**:
```python
# setup.py - EventStream created here
event_stream = EventStream(session_id, file_store)
runtime = runtime_cls(..., event_stream=event_stream, ...)

# base.py - Runtime.close() does NOT close EventStream
def close(self) -> None:
    pass  # EventStream is NOT closed!
```

**Resources leaked per inference**:
- `ThreadPoolExecutor` for each subscriber (creates threads)
- `asyncio.AbstractEventLoop` per subscriber (holds file descriptors)
- `threading.Thread` for queue processing
- `queue.Queue` for events

**Evidence** from `stream.py:56-70`:
```python
def __init__(self, sid: str, file_store: FileStore, ...):
    self._queue_thread = threading.Thread(target=self._run_queue_loop)
    self._queue_thread.daemon = True
    self._queue_thread.start()  # Thread created but never joined on close
```

### 2. LLMRegistry and LLM Have No Cleanup

**Location**: `openhands/llm/llm_registry.py` and `openhands/llm/llm.py`

**Problem**:
- `LLMRegistry` creates `LLM` instances stored in `service_to_llm` dict
- **NO `close()` method exists** for LLMRegistry or LLM
- Each inference run creates a new LLMRegistry with new UUID
- LLM uses litellm which creates httpx connections internally

**Evidence** from `llm_registry.py`:
```python
class LLMRegistry:
    def __init__(self, ...):
        self.registry_id = str(uuid4())  # New ID each time
        self.service_to_llm: dict[str, LLM] = {}
        # No close() method defined!
```

**Evidence** from `llm.py:444`:
```python
# Creates httpx connection that's never closed
response = httpx.get(f'{base_url}/v1/model/info', ...)
```

### 3. HttpSession.close() Doesn't Close Underlying Client

**Location**: `openhands/utils/http_session.py`

**Problem**:
```python
# Global singleton - NEVER closed
_client: httpx.Client | None = None

class HttpSession:
    def close(self) -> None:
        self._is_closed = True  # Only sets flag, doesn't close _client!
```

The global `_client` singleton accumulates connections but is never actually closed.

### 4. Docker Client Created Without Cleanup

**Location**: `openhands/runtime/impl/docker/containers.py`

**Problem**:
```python
def stop_all_containers(prefix: str) -> None:
    docker_client = docker.from_env()  # Creates new client each time
    try:
        # ... stop containers ...
    finally:
        docker_client.close()  # This is correct, but...
```

While `containers.py` does close the client, `DockerRuntime._init_docker_client()` uses `@lru_cache(maxsize=1)` which caches the client but never closes it on runtime shutdown.

### 5. atexit Handler Not Effective for Long-Running Processes

**Location**: `openhands/runtime/base.py:164`

```python
atexit.register(self.close)
```

This only runs at **program exit**, not when individual runtimes are closed. With 10+ runtimes created and "closed" but not garbage collected, the atexit handler has 10+ entries but they only execute when the Python interpreter shuts down.

## Resource Leak Summary Per Inference Run

| Resource | Location | Leaked |
|----------|----------|--------|
| EventStream | setup.py:63 | ThreadPoolExecutor, asyncio loops, threads |
| LLMRegistry | base.py, run_infer.py | LLM instances with httpx connections |
| HttpSession | action_execution_client.py:82 | Flag set but client not closed |
| Docker Client | docker_runtime.py:252 | Cached client never released |

## Estimated File Descriptors Per Sample

Based on typical usage:
- EventStream: ~4-6 FDs (threads, sockets)
- LLM connections: ~2-4 FDs (httpx to API providers)
- Docker client: ~2-3 FDs (socket connections)
- Runtime HTTP session: ~2-3 FDs

**Total per sample**: ~10-16 file descriptors leaked

**macOS default ulimit**: 256 open files
**Failure point**: ~10-20 samples (256 / 15 â‰ˆ 17)

This matches the observed failure at sample 11.

## Recommended Fixes

### Fix 1: Close EventStream in Runtime.close()

```python
# openhands/runtime/base.py
def close(self) -> None:
    if hasattr(self, 'event_stream') and self.event_stream:
        self.event_stream.close()
```

### Fix 2: Add close() Method to LLMRegistry

```python
# openhands/llm/llm_registry.py
def close(self) -> None:
    """Close all LLM instances and clean up resources."""
    for llm in self.service_to_llm.values():
        # If LLM had a close method
        pass
    self.service_to_llm.clear()
```

### Fix 3: Actually Close httpx Client in HttpSession

```python
# openhands/utils/http_session.py
def close(self) -> None:
    global _client
    self._is_closed = True
    # Don't close global client - but track session-specific resources
```

Or create per-session clients instead of a global singleton.

### Fix 4: Unregister atexit Handler on Proper Close

```python
# openhands/runtime/base.py
def __init__(self, ...):
    self._atexit_handler = lambda: self.close()
    atexit.register(self._atexit_handler)

def close(self) -> None:
    # Unregister to prevent accumulation
    try:
        atexit.unregister(self._atexit_handler)
    except Exception:
        pass
    # ... rest of cleanup
```

## Workaround for AWM (Immediate)

Increase the file descriptor limit before running AWM:

```bash
# macOS/Linux
ulimit -n 4096

# Or in the Python script
import resource
soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
resource.setrlimit(resource.RLIMIT_NOFILE, (4096, hard))
```

## Testing

After implementing fixes:

```bash
# Monitor file descriptors during test run
watch -n 1 'lsof -p $(pgrep -f "python.*awm") | wc -l'

# Or use this Python snippet in the loop
import os
fd_count = len(os.listdir(f'/proc/{os.getpid()}/fd'))
logger.info(f'Open FDs: {fd_count}')
```

## Priority

1. **High**: EventStream not closed - biggest leak source
2. **Medium**: LLMRegistry cleanup - causes gradual accumulation
3. **Low**: HttpSession singleton - single connection, minor impact

## Conclusion

The file descriptor leak is a systemic issue in OpenHands core where resources created during runtime initialization are not properly cleaned up when the runtime is closed. The fixes should be applied to OpenHands core, not AWM, as these resources are managed by the core infrastructure.

---

## Implementation (Completed)

The following fixes have been implemented:

### Fix 1: Runtime.close() Now Closes EventStream and LLMRegistry

**File**: `openhands/runtime/base.py`

```python
def _atexit_close(self) -> None:
    """Wrapper for atexit to call close. Separate method for unregistration."""
    self.close()

def close(self) -> None:
    """Close the runtime and release all resources."""
    if self._closed:
        return  # Prevent double-close
    self._closed = True

    # Unregister atexit handler to prevent accumulation
    try:
        atexit.unregister(self._atexit_close)
    except Exception:
        pass

    # Close EventStream to release ThreadPoolExecutors and asyncio loops
    if hasattr(self, 'event_stream') and self.event_stream is not None:
        try:
            self.event_stream.close()
        except Exception as e:
            logger.warning(f'Error closing event stream: {e}')

    # Close LLMRegistry to release LLM instances and their connections
    if hasattr(self, '_llm_registry') and self._llm_registry is not None:
        try:
            self._llm_registry.close()
        except Exception as e:
            logger.warning(f'Error closing LLM registry: {e}')
```

### Fix 2: LLMRegistry.close() Added

**File**: `openhands/llm/llm_registry.py`

```python
def close(self) -> None:
    """Close the registry and release all LLM resources."""
    self.subscriber = None
    self.service_to_llm.clear()
    logger.debug(f'[LLM registry {self.registry_id}]: Closed and cleared all LLMs')
```

### Fix 3: AWM CLI Now Checks and Adjusts File Descriptor Limits

**File**: `evaluation/awm/cli.py`

```python
def ensure_sufficient_file_descriptors(num_samples: int = 300) -> None:
    """Ensure sufficient file descriptors are available for AWM evaluation."""
    try:
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        min_needed = 50 + (num_samples * 25)
        recommended = max(min_needed, 8192)

        if soft < recommended:
            new_soft = min(recommended, hard) if hard > 0 else recommended
            resource.setrlimit(resource.RLIMIT_NOFILE, (new_soft, hard))
            # ... warnings if still insufficient
    except Exception as e:
        print(f"[FD Limit] Could not check file descriptor limits: {e}")
```

### Files Modified

| File | Changes |
|------|---------|
| `openhands/runtime/base.py` | Added `_closed` flag, `_atexit_close()`, updated `close()` to clean EventStream and LLMRegistry |
| `openhands/llm/llm_registry.py` | Added `close()` method |
| `evaluation/awm/cli.py` | Added `ensure_sufficient_file_descriptors()` function |

### Testing

- All EventStream unit tests pass (22/22)
- All LLM unit tests pass (199/199)
- Import validation successful
- FD check function works correctly

### Expected Result

With these fixes:
- EventStream resources (ThreadPoolExecutors, asyncio loops) are properly released
- LLMRegistry clears all LLM references, allowing garbage collection of httpx connections
- atexit handlers don't accumulate
- AWM CLI automatically increases FD limit or warns if insufficient

The combination of proper cleanup + automatic FD limit management should allow running 300+ samples without file descriptor exhaustion.
